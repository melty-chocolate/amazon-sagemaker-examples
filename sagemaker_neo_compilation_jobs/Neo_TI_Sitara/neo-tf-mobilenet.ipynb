{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TI Sitara x AWS NEO Image Classification Example\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data preparation](#Data-preparation)\n",
    "3. [Training the MobileNet V2 model](#Training-the-MobileNet-V2-model)\n",
    "4. [Compiling the model using NEO](#Compiling-the-model-using-NEO)\n",
    "5. [Deploy the trained model to TI Sitara for inference](#Deploy-the-trained-model-to-TI-Sitara-for-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will demo how to train TensorFlow Mobilenet V2 model in Sagemaker, compile using the Neo API backend, to optimize for TI Sitara AM57xx. Finally, we can deploy compiled model to device and do inference using the Neo Deep Learning Runtime.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services.\n",
    "\n",
    "* The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "* The S3 bucket that you want to use for training and model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import get_execution_role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role() \n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "We will use the [Caltech101](http://www.vision.caltech.edu/Image_Datasets/Caltech101) dataset in this example. Inside this dataset, pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc 'Aurelio Ranzato.  The size of each image is roughly 300 x 200 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf 101_ObjectCategories.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use `.tfrecord` as input. The TFRecord format is a simple format for storing a sequence of binary records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob, imageio, shutil, os\n",
    "\n",
    "# Gather file paths to all iamges\n",
    "data_dir = '101_ObjectCategories'\n",
    "object_dirs = glob.glob(data_dir + '/*')\n",
    "\n",
    "objects = {}\n",
    "for d in object_dirs:\n",
    "    objects[d.split('/')[1]] = glob.glob(d + '/*.jpg')\n",
    "\n",
    "# Create an integer label for each object category\n",
    "categories = list(objects.keys())\n",
    "category_labels = {}\n",
    "for i in range(len(categories)):\n",
    "    category_labels[categories[i]] = i\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def float_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "# Create train/valid directories to store our TFRecords\n",
    "if not os.path.exists('tfrecords') and not os.path.isdir('tfrecords'):\n",
    "    os.mkdir('tfrecords')\n",
    "\n",
    "object_names = list(objects.keys())\n",
    "# Create a separate TFRecord file for each object category\n",
    "train_writer = tf.io.TFRecordWriter('tfrecords/' + 'train.tfrecord')\n",
    "valid_writer = tf.io.TFRecordWriter('tfrecords/' + 'valid.tfrecord')\n",
    "for o in object_names:\n",
    "    # Write each image of the object into that file\n",
    "    num_images = len(objects[o])\n",
    "    for index in range(num_images):\n",
    "        i = objects[o][index]\n",
    "        # Let's make 80% train and leave 20% for validation\n",
    "        if index < num_images * 0.8:\n",
    "            writer = train_writer\n",
    "        else:\n",
    "            writer = valid_writer\n",
    "        image = imageio.imread(i)\n",
    "        shape = image.shape\n",
    "        if len(shape) != 3:\n",
    "            continue\n",
    "        label = category_labels[o]\n",
    "        # Create features dict for this image\n",
    "        features = {\n",
    "            'height' : int64_feature(shape[0]),\n",
    "            'width' : int64_feature(shape[1]),\n",
    "            'depth' : int64_feature(shape[2]),\n",
    "            'image' : bytes_feature(image.tostring()),\n",
    "            'label' : int64_feature(int(label))\n",
    "        }\n",
    "        # Create Example out of this image and write it to the TFRecord\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(example.SerializeToString())\n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sess.upload_data(path='tfrecords', key_prefix='data/DEMO-caltech101')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MobileNet V2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training job using the ``sagemaker.TensorFlow`` estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "import os\n",
    "\n",
    "source_dir = os.path.join(os.getcwd(), 'source_dir')\n",
    "estimator = TensorFlow(entry_point='mobilenet_v2_training.py',\n",
    "                       source_dir=source_dir,\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       hyperparameters={'throttle_secs': 30},\n",
    "                       training_steps=1000, \n",
    "                       evaluation_steps=100,\n",
    "                       train_instance_count=2, \n",
    "                       train_instance_type='ml.p3.2xlarge', \n",
    "                       base_job_name='demo-ti-neo'\n",
    "                      )\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is ready to be compiled by Neo to be optimized for our hardware of choice. We are using the  ``TensorFlowModel.compile`` method to do this. For this example, our target hardware is ``'sitara'``. You can changed these to other supported target hardware if you prefer.\n",
    "\n",
    "### Compiling the model using NEO\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**\n",
    "\n",
    "After compiling, the compiled model will be stored in a S3 bucket as a tarball file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_path = 's3://{}/tf-mobilenet/output'.format(bucket)\n",
    "compiled_mobilenet = estimator.compile_model(target_instance_family='sitara_am57x', \n",
    "                                             input_shape={'input':[1,224,224,3]},\n",
    "                                             role=role,\n",
    "                                             framework='tensorflow',\n",
    "                                             framework_version='1.12.0',\n",
    "                                             output_path=output_path,\n",
    "                                             compile_max_run=15 * 60\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model to TI Sitara for inference\n",
    "\n",
    "***Please execute the following on a sitara device.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download compiled model from S3 to device\n",
    "\n",
    "After compilation using Neo, we have a optimized model stored in S3 bucket. Now download the compiled model, and then deploy it use Neo DLR runtime on device.\n",
    "\n",
    "Before we can access the S3 bucket which has the compiled model, we need to configure the aws account credentials beforehand. Please refer to [AWS Security Credentials](https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "object_path = prefix+'/output/mobilenet_v2_1.0_224-sitara_am57x.tar.gz'\n",
    "s3.download_file(bucket, object_path, 'mobilenet_v2.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvf mobilenet_v2.tar.gz -C ./mobilenet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DLR to read compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from dlr import DLRModel\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = \"./mobilenet_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = DLRModel(model_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download an image to prepare for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display input image\n",
    "!wget -O /tmp/test.jpg http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/080.frog/080_0001.jpg\n",
    "file_name = '/tmp/test.jpg'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image pre-process\n",
    "\n",
    "MobileNet V2 has a fixed input shape which is (3, 224, 224), and in order to utilize 4 EVEs on Sitara Am57XX batch size 4 is the minimal requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "image = PIL.Image.open(file_name)\n",
    "\n",
    "# Resize\n",
    "image = np.asarray(image.resize((224, 224)))\n",
    "\n",
    "# Normalize\n",
    "image = image*2/255.0 - 1\n",
    "\n",
    "image = np.concatenate([image[np.newaxis, :, :]]*batch_size)\n",
    "\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten within a input array\n",
    "input_data = {'input': image}\n",
    "\n",
    "print('Testing inference on mobilenete_v2...')\n",
    "start_time = time.time()\n",
    "out = model.run(input_data) #need to be a list of input arrays matching input names\n",
    "index = np.argmax(out[0][0,:])\n",
    "prob = np.amax(out[0][0,:])\n",
    "print('inference time is ' + str((time.time()-start_time)/batch_size) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names for ImageNet classes\n",
    "object_categories = {}\n",
    "with open(\"imagenet1000_clsidx_to_labels.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.strip().split(':')\n",
    "        object_categories[key] = val\n",
    "\n",
    "print(\"Result: label - \" + object_categories[str(index)] + \" probability - \" + str(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
